{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Executor**\n",
    "\n",
    "O executor trabalha em estreita colaboração com o _scheduler_ e são o mecanismo pelo qual as instâncias de tarefas são executadas. Eles têm uma API comum e são \"plugáveis\", o que significa que é possível trocar os executores com base nas necessidades da instalação. O Airflow possui vários executores, mas só é possível utilizar **um por vez**. Eles são configurados no arquivo de configuração na opção [core]. \n",
    "\n",
    "Existem dois tipos de executores. A diferença entre eles se resume aos recursos que possuem e à forma como trabalham na execução das tasks. **Os executores são divididos em: executores locais e os executores remotos**. \n",
    "\n",
    "**Executores locais:**\n",
    "\n",
    "- Executor Sequencial;\n",
    "- Executor Local.\n",
    "\n",
    "**Executores remotos:**\n",
    "\n",
    "- Executor Celery;\n",
    "- Executor Kubernetes;\n",
    "- Executor CeleryKubernetes;\n",
    "- Executor LocalKubernetes;\n",
    "- Executor Dask.\n",
    "\n",
    "**SequentialExecutor:**\n",
    "\n",
    "O **SequentialExecutor** (ou Executor Sequencial) realiza somente uma tarefa por vez, de forma linear, sem funcionalidade de paralelismo, e por isso é o mais simples. Ele utiliza um banco de dados chamados SQLite que aceita apenas uma conexão por vez.\n",
    "\n",
    "> O SequentialExecutor faz parte da configuração padrão do Airflow.\n",
    "\n",
    "**LocalExecutor:**\n",
    "\n",
    "O **LocalExecutor** (ou Executor Local) roda localmente na máquina e executa várias instâncias de tarefas por vez, com a funcionalidade de paralelismo, ou seja, realiza várias tarefas simultaneamente. Mas, para que possa ser utilizado, é preciso alterar algumas configurações do Airflow, dentre elas o banco de dados. Como este executor realiza várias tarefas ao mesmo tempo, necessita de um banco de dados que permita várias conexões de forma simultânea. Na documentação do Airflow, é sugerido o uso do banco de dados Ocean ou MySQL.\n",
    "\n",
    "As maiores vantagens do executor Local são: a fácil configuração e o paralelismo - execução de múltiplas tarefas simultaneamente. A desvantagem seria a dependência do funcionamento de uma única máquina, pois, por rodar localmente, terá apenas um ponto de falha, sendo assim, se a máquina vier a ter algum problema, as tarefas executadas também terão. Este mecanismo é utilizado principalmente para executar tarefas ou DAGs paralelamente e para realizar testes, no entanto, por depender de um único ponto de falha, não é recomendado para a produção, a menos que estas sejam leves.\n",
    "\n",
    "**Alterações para usar o LocalExecutor:**\n",
    "\n",
    "Dentro do airflow.cfg é necessário alterar a string de conexão SqlAlchemy para uma database, pois ela aceita mais de uma conexão simultânea, como o Postgres.\n",
    "\n",
    "**Configurações de Paralelismo:**\n",
    "\n",
    "Dentro do airflow.cf existe o parâmetro `max_active_tasks_per_dag`, definido por padrão como \"16\". Ele é utilizado para definir a quantidade máxima de tarefas que podem ser executadas ou agendadas simultaneamente em um DAG. Ainda, há o `max_active_runs_per_dag`, também definido como \"16\", que estabelece a quantidade máxima de DAG runs que podem ser executados simultaneamente em um mesmo DAG. DAG run é basicamente a instância de execução do DAG no tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alterando as configurações necessárias para utilizar o executor local:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso, é necessário fazer a instalação do banco de dados Postgres e criar um novo banco de dados utilizando os seguintes comandos:\n",
    "\n",
    "- Atualizando os pacotes do Ubuntu:\n",
    "\n",
    "`sudo apt update`\n",
    "`sudo apt upgrade`\n",
    "\n",
    "- Instalando o Postgres:\n",
    "\n",
    "`sudo apt install postgresql postgresql-contrib`\n",
    "\n",
    "- Acessando o usuários postgres:\n",
    "\n",
    "`sudo -i -u postgres`\n",
    "\n",
    "- Criando um banco de dados:\n",
    "\n",
    "`createdb airflow_db`\n",
    "\n",
    "- Acessando o banco de dados criado:\n",
    "\n",
    "`psql airflow_db`\n",
    "\n",
    "- Criando um usuário e uma senha para o banco de dados:\n",
    "\n",
    "`CREATE USER airflow_user WITH PASSWORD 'airflow_pass';`\n",
    "\n",
    "- Garantindo privilégios para o usuários criado ao banco de dados:\n",
    "\n",
    "`GRANT ALL PRIVILEGES ON DATABASE airflow_db TO airflow_user;`\n",
    "\n",
    "Além disso, é possível alterar os seguintes parâmetros no arquivo de configuração do **airflow.cfg**:\n",
    "\n",
    "- Parâmetro executor:\n",
    "\n",
    "`executor = LocalExecutor`\n",
    "\n",
    "- Parâmetro sql_alchemy_conn:\n",
    "\n",
    "`sql_alchemy_conn = postgresql+psycopg2://airflow_user:airflow_pass@localhost/airflow_db`\n",
    "\n",
    "- Parâmetro max_active_per_dags:\n",
    "\n",
    "`max_active_tasks_per_dag = 2`\n",
    "\n",
    "-Parâmetro max_active_runs_per_dag:\n",
    "\n",
    "`max_active_runs_per_dag = 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao utilizar o executor Local, existe a limitação de recursos de uma única máquina para executar nossas tarefas, o que não é indicado em casos que de DAG em produção, justamente porque, em caso de falha na máquina, toda a execução das tarefas também falhará. No entanto, o Airflow oferece uma alternativa para lidar com este problema: o **executor Celery**. \n",
    "\n",
    "Este executor permite trabalhar com fila de tarefas, mecanismo que admite a distribuição dessas tarefas em processos distintos, sendo cada um executado por diferentes **workers**. Os **workers funcionam como máquinas individuais** que aguardam as tarefas chegarem na fila para executá-las.\n",
    "\n",
    "O **Celery permite** criar workers em máquinas diferentes, gerando o **processamento distribuído de tarefas**, o que resolve o problema de depender unicamente de um ponto de falha. Sendo assim, caso uma máquina venha a falhar, somente um worker também falhará, e as tarefas executadas por ele serão enviadas para um worker que permanece em funcionamento, impedindo uma quebra no fluxo de execução dessas tarefas.\n",
    "\n",
    "Após a configuração do executor, o processamento se fecha em uma espécie de ciclo. Este **ciclo inicia com o Scheduler**, responsável por enviar as tarefas agendadas para a fila. O **Worker aguarda a chegada dessas tarefas na fila para que possa executá-las** e, ao **final da execução, reporta ao banco de dados**. O **banco de dados**, por sua vez, **registra a tarefa e seus status**. **Por fim, o Scheduler lê esse banco de dados e atualiza o status da tarefa no dag**. Esse processo é o mesmo independentemente da quantidade de workers.\n",
    "\n",
    "Vale ressaltar, no entanto, que para o funcionamento desse processo, o **Celery precisa de um broker (\"mediador\"), responsável por armazenar as tarefas que estão sendo executadas**. Este mediador é utilizado, basicamente, para guardar a fila de tarefas. Na documentação do Celery, há duas indicações de banco de dados não relacionais que podem ser adotados para essa função. Nesse projeto, será utilizado o Redis.\n",
    "\n",
    "As principais vantagens do CeleryExecutor é que ele oferece paralelismo (execução de várias tarefas ao mesmo tempo), alta disponibilidade (capacidade de utilizar várias máquinas evitando um único ponto de falha), e processamento distribuído das tarefas. As desvantagens ficam por conta da configuração mais trabalhosa e da manutenção do Worker, porque, havendo mais de um worker, faz-se necessário configurar todas as máquinas. Por sua performance, este executor é utilizado principalmente em cargas de trabalho mais pesadas e na colocação de DAGs em produção.\n",
    "\n",
    "Fazendo o **Celery** funcionar: \n",
    "\n",
    "`pip install 'apache-airflow[celery]'`\n",
    "\n",
    "- Baixar o seguinte arquivo: \n",
    "\n",
    "https://caelum-online-public.s3.amazonaws.com/2606-aprofundando-airflow-executor-celery/01/redis-7.0.4.tar.gz\n",
    "\n",
    "- Abrir o repositório onde baixou o arquivo (no caso deixei em 'Documents');\n",
    "\n",
    "- Descompactar o arquivo:\n",
    "\n",
    "`tar -xf redis-7.0.4.tar.gz`\n",
    "\n",
    "- Acessar o repositório descompactado:\n",
    "\n",
    "`cd redis-7.0.4/`\n",
    "\n",
    "- O _Redis_ possui módulos que precisar ser instalados, para isso executar o comando:\n",
    "\n",
    "`make`\n",
    "\n",
    "> A execução pode demorar algum tempo.\n",
    "\n",
    "- Finalizar a execução do comando anterior com:\n",
    "\n",
    "`sudo make install`\n",
    "\n",
    "- Para verificar se a instalação foi concluída com sucesso pode executar o comando:\n",
    "\n",
    "`redis-server`\n",
    "\n",
    "**Nessa etapa o log retorna uma série de informações, entre elas a porta (que deve aparecer como \"Port\") onde o _Redis_ está rodando. Essa informação será necessária.**\n",
    "\n",
    "Voltando para as configurações do airflow.cfg:\n",
    "\n",
    "- Alterar o executor do arquivo:\n",
    "\n",
    "`executor = CeleryExecutor`\n",
    "\n",
    "- Antes do [core] section adicionar:\n",
    "\n",
    "```\n",
    "[celery]\n",
    "broker_url = redis://localhost:6379/0\n",
    "result_backend = db+postgresql://airflow_user:airflow_pass@localhost/airflow_db\n",
    "worker_concurrency = 4  # Adjust based on your system's capacity\n",
    "\n",
    "[celery_broker_transport_options]\n",
    "visibility_timeout = 3600  # Optional: Adjust based on your task durations\n",
    "```\n",
    "//\n",
    "\n",
    "- Buscar por \"result_backend\". Aqui se trata de uma variável que recebe uma url correspondente ao endereço do banco de dados responsável por armazenar o status das tarefas, neste caso, o Postgres. Por ser semelhante ao passado para a variável \"sql_alchemy\", é possível deixar o trecho inicial da url, \"db+postgresql:\", e alterar o restante pelo nome do usuário (\"airflow_user\"), senha (\"airflow_pass\") e nome do banco (\"airflow_db\") que foi criado no Postgres. Dessa forma, ficará assim:\n",
    "\n",
    "`result_backend = db+postgresql://airflow_user:airflow_pass@localhost/airflow_db`\n",
    "\n",
    "- Agora é precisa alterar o parâmetro \"broker_url\", onde será passado o endereço do _Redis_, ou seja, onde está sendo executado. Para isso, coloca o localhost, uma vez que está rodando localmente na máquina, e a porta informada anteriormente no log.\n",
    "\n",
    "`broker_url = redis://0.0.0.0:6379/0`\n",
    "\n",
    "- Salvar o arquivo.\n",
    "\n",
    "A partir disso, para utilizar o Airflow, será preciso de um terminal executando o banco de dados Redis a fim de que o Airflow consiga se conectar a ele em tempo de execução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O **Redis é um banco de dados não relacional do tipo chave-valor**, que é uma forma de armazenarmos um valor vinculado a uma chave. Ele é um sistema voltado para armazenamento e processamento mais dinâmico e ágil. Além disso, esse banco de dados pode ser utilizado em diferentes casos de uso como armazenamento de cache, streaming de mídia, dentre outros.\n",
    "\n",
    "O Celery utiliza o Redis como mediador (broker), pois ele é responsável por \"mediar\" as tarefas que são enviadas pelo scheduler até o worker. Sendo assim, **ele recebe as tarefas do Scheduler**, **armazena elas na fila de tarefas** e **entrega essas tarefas para um worker que será responsável por executá-las**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não é possível usar o comando airflow standalone para executar o Airflow, porque este só é utilizado em situações onde se usa o executor sequencial ou local. Se executar dessa forma, as configurações assumirão a formatação anterior, correspondente a estes executores. Sendo assim, é **necessário executar cada um dos componentes separados do Airflow**: **um terminal será para o Airflow Scheduler** e outro para o Airflow **web server**:\n",
    "\n",
    "Em outro terminal:\n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "`export AIRFLOW_HOME=$(pwd)/airflow_pipeline`\n",
    "\n",
    "`airflow scheduler`\n",
    "\n",
    "E em novo terminal:\n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "`export AIRFLOW_HOME=$(pwd)/airflow_pipeline`\n",
    "\n",
    "`airflow webserver`\n",
    "\n",
    "O Celery possui uma ferramenta chamada Flower que permite visualizar as tarefas que estão na fila, aguardando os workers pegá-las e executá-las.\n",
    "\n",
    "Para ativar acessar essa ferramenta é preciso abrir mais um terminal e executar o comando:\n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "`export AIRFLOW_HOME=$(pwd)/airflow_pipeline`\n",
    "\n",
    "`airflow celery flower`\n",
    "\n",
    "O log de execução deste comando trará várias informações, entre elas o endereço de acesso \"http://0.0.0.0:5555\", que deve vir antecedido pelo texto \"Visit me at\". \n",
    "\n",
    "Na interface principal do _Flower_ há uma aba chamada \"Dashboard\" que traz informações sobre os workers que estão rodando e as tarefas que estão sendo executadas por eles. A terceira aba, chamada \"Broker\", diz respeito ao número de tarefas que estão na fila para serem executadas.\n",
    "\n",
    "Voltando ao terminal e abrindo uma nova aba é precisso realizar o processo de ativar o ambiente e importar a variável:\n",
    "\n",
    "`source venv/bin/activate`\n",
    "\n",
    "`export AIRFLOW_HOME=$(pwd)/airflow_pipeline`\n",
    "\n",
    "Para acionar o worker é necessário usar o comando:\n",
    "\n",
    "`airflow celery worker`\n",
    "\n",
    "Voltando para a interface do Airflow e é possível observar que os cubos que representavam as tarefas na fila, antes verde escuro, agora indicam que essas tarefas estão em execução, assumindo a cor verde claro. \n",
    "\n",
    "Na página do Flower e aparece que as mensagens da aba \"Broker\" foram zeradas, não há mais nenhuma tarefa aguardando execução porque o worker já está a executá-las. Na aba \"Dashboard\" e consta um worker ativo, com status online, seguido da quantidade de tarefas em execução - número que varia entre 0 e 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configurações do arquivo airflow.cfg:**\n",
    "\n",
    "- `max_active_tasks_per_dag`: limita a quantidade de Dag runs executados simultaneamente;\n",
    "- `max_active_tasks_per_task`: limita a quantidade de tarefas executadas simultaneamente;\n",
    "- `worker_concurrency`: limita o número de tarefas simultâneas que cada worker pode exeutar.\n",
    "\n",
    "Além desses ajustes, existe outro parâmetro de os pools, que funcionam como \"reservatórios\". Por exemplo, no exemplo usando o yfinance existem duas dags: get_stocks e get_crypto. Ambas fazem a extração de dados da API do yfinance de forma simultânea. **É preciso tomar cuidado com o fato de que a API em questão possui um número limite de requisições** que ela aceita receber de forma conjunta. Se durante a execução das tasks, esse limite for ultrapassado falhas irão ocorrer nas execuções das tasks.\n",
    "\n",
    "A configuração do _pool_ pode ser utilizada para limitar o parelelismo de execução em conjuntos específicos de tarefas. Esse ajuste informa **quantas tasks podem ser executadas de forma paralela considerando todos os dags ativos ao mesmo tempo**. Quando existem muitas tarefas que interagem com o mesmo sistema de origem, como um banco de dados ou API, não é interessante sobrecarregá-los com requisições. É justamente em situações como essas que são utilizados os pools.\n",
    "\n",
    "**Criando uma _pool_** \n",
    "\n",
    "> Na interface do Airflow > Admin > Pools \n",
    "\n",
    "Ela o levará ao reservatório padrão \"default_pool\" e à quantidade de slots que ele disponibiliza para a execução de tarefas, que está definida como \"128\". \n",
    "\n",
    "**Cada tarefa utiliza 1 slot enquanto está em execução**, então a quantidade de slots disponibilizadas pelo pool condiz com o número de tarefas que podem ser executadas simultaneamente, considerando todos os DAGs ativos que utilizam o mesmo reservatório. Sendo assim, quando não tem nenhum outro pool, todos os DAGs criados utilizam o reservatório \"default_pool\" por padrão.\n",
    "\n",
    "> **+** > Add a new record > nome: 'qualquer_nome_pool' \n",
    "\n",
    "Depois disso, é preciso especificar quais dags vão utilizar esse pool criado e à medida que os slots ficam disponíveis novamente, as tasks que restaram na fila de execução começam a ser executadas.\n",
    "\n",
    "No caso, essa especificação é feito no código do dag (get_stocks.py e get_crypto.py), passada como parâmetros no método override()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Executor Kubernetes**:\n",
    "\n",
    "O Kubernetes é um orquestrador de containers, ou seja, ele é utilizado para lidar com um conjunto de containers. Ele consegue gerenciar um cluster com várias máquinas e, por conta da sua arquitetura, se torna uma ferramenta muito interessante.\n",
    "\n",
    "O Kubernetes gerencia um cluster basicamente dividido por: um master que gerencia o cluster e mantém tudo no estado desejado; e os nodes (ou nós), que costumamos chamar de workers (ou trabalhadores), que são responsáveis pela execução da aplicação.\n",
    "\n",
    "Dessa forma, para conseguir utilizar o executor kubernetes da melhor forma, é preciso configurar o Airflow em um ambiente kubernetes. O primeiro passo, portanto, é preparar todo o ambiente kubernetes na máquina local, começando pela instalação de um gerenciador de máquinas virtuais, o Docker. É importante realizar a instalação do gerenciador de máquinas virtuais, porque o kubernetes precisa dele para funcionar \"por baixo dos panos\". \n",
    "\n",
    "Instalação do Docker:\n",
    "\n",
    "- Caso exista o interesse em remover o que já estava instalado:\n",
    "\n",
    "`sudo apt-get remove docker docker-engine docker.io containerd runc`\n",
    "\n",
    "- Atualizar os pacotes da máquina:\n",
    "\n",
    "`sudo apt-get update`\n",
    "\n",
    "- Instalações de alguns pacotes recomendados:\n",
    "\n",
    "`sudo apt-get install ca-certificates curl gnupg lsb-release`\n",
    "\n",
    "- Configuração da chave oficial do Docker:\n",
    "\n",
    "`sudo mkdir -p /etc/apt/keyrings`\n",
    "\n",
    "- Adicionar a chave oficial do Docker:\n",
    "\n",
    "`curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg`\n",
    "\n",
    "- A chave foi adicionada e podé possível executar o comando para a configuração do repositório do Dcoker na máquina local:\n",
    "\n",
    "```\n",
    "echo \\\n",
    "  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n",
    "  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "``` \n",
    "\n",
    "- Por fim, executar novamente o comando de atualização de pacotes:\n",
    "\n",
    "`sudo apt-get update`\n",
    "\n",
    "- Execuutar o comando de instalação do Docker:\n",
    "\n",
    "`sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin`\n",
    "\n",
    "- Iniciar o Docker:\n",
    "\n",
    "`sudo service docker start`\n",
    "\n",
    "- Para conferirmos se a instalação está correta, é possível executar uma imagem Docker chamada \"Hello World\", apenas ao nível de teste:\n",
    "\n",
    "`sudo docker run hello-world` \n",
    "\n",
    "- Adicionar um grupo para que o Kubernetes possa utilizar o Docker (usar sem sudo antes*):\n",
    "\n",
    "`sudo groupadd docker`\n",
    "\n",
    "- **Se já existe um grupo, é possível usar esse comando para adicionar o usuário ao grupo docker:**\n",
    "\n",
    "`sudo usermod -aG docker $USER`\n",
    "\n",
    "- Testar se as configurações foram devidamente aplicadas executando novamente o comando da imagem \"Hello World\", desta vez, sem o sudo:\n",
    "\n",
    "`docker run hello-world`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar um ambiente Kubernetes na máquina local, é possível utilizar uma ferramenta chamada Minikube - uma das formas mais simples de configurar um cluster Kubernetes na máquina local, utilizada principalmente para realização de testes de aplicações.\n",
    "\n",
    "#### **Instalação do Minikube**\n",
    "\n",
    "- Garantir que o Docker esteja rodando antes de inciar a instalação do Minikube:\n",
    "\n",
    "`sudo service docker start`\n",
    "\n",
    "- Istalação do Minikube, seguindo a documentação da própria ferramenta:\n",
    "\n",
    "`curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64`\n",
    "\n",
    "`sudo install minikube-linux-amd64 /usr/local/bin/minikube`\n",
    "\n",
    "- Checar se instalação:\n",
    "\n",
    "`minikube start`\n",
    "\n",
    "- Quando finalizado, conferir se o Minikube foi inicializado e o cluster Kubernetes configurado através do comando minikube status:\n",
    "\n",
    "`minikube status`\n",
    "\n",
    "> Essa execução deve retornar uma mensagem semelhante à: **minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured**\n",
    "\n",
    "- Para executar comandos do Kubectl dentro do minikube, se usa a estruturação minikube kubectl -- e o comando que desejamos executar:\n",
    "\n",
    "`minikube kubectl --help`\n",
    "\n",
    "- Com o minikube instalado, é possível acessar o dashboard do Kubernetes para visualizar, configurar e gerenciar o cluster. Para isso, é preciso executar o comando minikube dashboard que deve retornar, entre outras informações, uma URL pela qual acessa o dashboard:\n",
    "\n",
    "`minikube dashboard`\n",
    "\n",
    "Acessando a URL pelo navegador, estará o dashboard do Kubernetes, onde é possível gerenciar todos os serviços que estão rodando. Na barra lateral esquerda, em \"Nodes\", na seção de \"Cluster\" é possível ver que tem um único node rodando, no caso, o minikube, então é preciso acessa-lo. Feito isso, terá acesso a algumas informações sobre este node, além da alocação de recursos da máquina local. Novamente na barra lateral, ao acessar \"Pods\" é visto que não tem nenhum pods rodando, mas, posteriormente, quando for instalado e configurado o Airflow no Kubernetes, terão alguns pods em execução responsáveis pela execução dos serviços do Airflow e das tarefas dos DAGs.\n",
    "\n",
    "- Parando o cluster Kubernetes:\n",
    "\n",
    "`minikube stop`\n",
    "\n",
    "#### **Configurando o Helm:**\n",
    "\n",
    "Para finalizar essa etapa de configurar um cluster Kubernets é preciso configurar o _Helm_, que é um gerenciador de pacotes para Kubernetes. Da mesma forma que existe o Docker para gerenciar as imagens Docker e o PyPi para gerenciar as biblioteca Python, existe o Helm para gerenciar os pacotes para Kubernetes. Utilizando o Helm é possível instalar, atualizar ou desinstalar pacotes para aplicativos Kubernetes de uma forma bem mais simples e automatizada, utilizando apenas alguns comandos. Para instalar aplicativos no Kubernetes sem a utilização do Helm, seria preciso definir todas as configurações manualmente utilizando um arquivo YAML bem detalhado.\n",
    "\n",
    "Os **pacotes do Helm são chamados de charts e o Airflow possui um chart específico** que pode ser utilizado para realizar a configuração de um ambiente Airflow dentro do Kubernetes. Mas para que seja possível utilizar esse chart, é preciso instalar e configurar o Helm na máquina local.\n",
    "\n",
    "Para isso é preciso seguir os seguintes comandos:\n",
    "\n",
    "- Comando para baixar o script de instalação do Helm:\n",
    "\n",
    "`curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3`\n",
    "\n",
    "- Conceder algumas permissões:\n",
    "\n",
    "`chmod 700 get_helm.sh`\n",
    "\n",
    "- Executar o script baixado:\n",
    "\n",
    "`./get_helm.sh`\n",
    "\n",
    "- Testar se a instalação foi realizado com sucesso -> pesquisar o chart do Airflow:\n",
    "\n",
    "`helm search hub apache-airflow`\n",
    "\n",
    "- Adiciona o chart do Airflow ao repositório do Helm para que possa ser utilizado posteriormente:\n",
    "\n",
    "`helm repo add apache-airflow https://airflow.apache.org`\n",
    "\n",
    "- Conferir se foi devidamente adicionado ao repositório:\n",
    "\n",
    "`helm search repo apache-airflow`\n",
    "\n",
    "**Versões instaladas de cada ferramenta seguindo esses comandos:**\n",
    "\n",
    "- Docker: 20.10.20\n",
    "- Minikube: 1.28.0\n",
    "- Helm: 3.10.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Celery X Kubernetes Executor**\n",
    "\n",
    "#### Tarefas sensíveis ao horário de execução\n",
    "\n",
    "Em situações que existem tarefas sensíveis ao horário de execução, o executor Kubernetes não é tão interessante de ser utilizado. Isso porque, utilizando esse executor, as tarefas podem demorar até alguns minutos para começar a executar, pois o Airflow precisa esperar o tempo para que um novo pod seja instanciado no cluster toda vez que uma nova tarefa é executada. Isso significa que recursos precisam ser alocados e uma nova imagem do Airflow precisa ser configurada e iniciada naquele container, para então a tarefa ser executada.\n",
    "\n",
    "Já utilizando o executor Celery, as tarefas são executadas quase instantaneamente após serem disparadas pelo Scheduler. Isso porque os workers do Celery estão sempre ativos e esperando as tarefas para serem executadas.\n",
    "\n",
    "#### DAGs com frequência de execução não contínua\n",
    "\n",
    "Por outro lado, caso os DAGs tenham uma frequência de execução não contínua ou bastante esparsa, recursos podem ser economizados ao utilizar o executor Kubernetes. Isso acontece, pois ele consegue escalonar os Pods para zero, já que os Pods são iniciados somente quando há tarefas a serem executadas, enquanto no Celery os workers ficam executando continuamente esperando por tarefas.\n",
    "\n",
    "Sendo assim, pode-se dizer que o principal motivo para utilizar o executor Kubernetes é focado na possibilidade de configurar recursos que serão alocados para cada tarefa, algo que não é possível de ser feito no Celery. Com o Kubernetes é possível especificar por tarefa a quantidade de CPU e memória que serão alocados para o pod, assim como volumes e imagens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configurando volumes persistentes**: definindo as configurações dos volumes para serem usadas pelo _minikube_ para aplicar e criar os volumes persistentes no _cluster kubernetes_.\n",
    "\n",
    "Armazenamentos locais, em containers ou pods, são efêmeros, ou seja, eles têm uma vida curta junto ao container. Isso significa que quando o container é encerrado, todos os dados são apagados com ele.\n",
    "\n",
    "Desta maneira, se deseja que os dados estejam desacoplados do container, que significa que mesmo quando o container for encerrado, os dados permaneçam salvos, é preciso criar os volumes persistentes ou PV (Persistent Volume).\n",
    "\n",
    "Os volumes persistentes são uma parte do armazenamento do cluster. É como se fossem criadas pastas dentro do cluster para salvarm e armazenar dados, além de compartilhar informações entre os pods.\n",
    "\n",
    "**Os PVCs, de Persistent Volume Claim, são os objetos de reivindicação de volume**. Eles são responsáveis por \"pegar\" um pedaço do armazenamento disponível em um Persistent Volume e disponibilizar para algum Pod específico. Ou seja, um PVC é uma forma de um Pod ter acesso a um determinado pedaço de armazenamento que foi proporcionado por um PV específico. \n",
    "\n",
    "No caso do cluster do projeto em questão, será importante ter, no mínimo, três volumes persistentes:\n",
    "\n",
    "- **airflow-dags-volume: armazenar os arquivos de DAGs**\n",
    "- **airflow-logs-volume: arquivar os logs gerados pelo Airflow quando for executado**\n",
    "- **airflow-data-volume: armazenar os arquivos que forem extraídos pelo DAG**\n",
    "\n",
    "Para criar esses volumes, é necessário definir um arquivo YML, contendo todas as configurações desejadas para eles. Nessa etapa é interessante criar uma pasta na máquina local, para organizar todos os arquivos do projeto em subpastas dentro dela. \n",
    "\n",
    "**Definindo o arquivo YML:**\n",
    "\n",
    "> YAML é uma notação de dados legível por humanos que permite representar estruturas de dados simples e aninhadas em um formato de texto. A sintaxe básica consiste em elementos chva-valor, onde cada chave é seguida por um valor e separada por dois-pontos (:). Esses elementos são geralmente organizados em blocos, em que cada bloco representa uma estrutura de dados diferente.\n",
    "\n",
    "- Criar um novoarquivo na pasta criada. Exemplo: 'airflow-config.yml';\n",
    "\n",
    "- Código das configurações estão nesse arquivo criado que será definido sete objetos, cada um separado por (---). \n",
    "\n",
    "- **Primeiro objeto:**\n",
    "\n",
    "```\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: airflow\n",
    "  labels:\n",
    "    name: airflow\n",
    "```\n",
    "\n",
    "Com o primerio objeto, será criado um namespace para instalar os serviços do airflow. Namespace é, basicamente, um grupo de recursos isolados para fazer algo específico no Cluster Kubernetes. No caso está criando um namespace, onde ficarão todos os serviços do airflow. \n",
    "\n",
    "Foi definido o `kind`, que é o tipo do objeto: `Namespace`. Abaixo, em metadata, foi definido também o nome do _namespace_: `airflow`.\n",
    "\n",
    "- **Segundo, quarto e sexto objetos:**\n",
    "\n",
    "```\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: airflow-dags-volume\n",
    "  labels:\n",
    "    type: local\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  capacity:\n",
    "    storage: 1Gi\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  hostPath:\n",
    "    path: \"/data/dags\"\n",
    "\n",
    "---\n",
    "\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: airflow-logs-volume\n",
    "  labels:\n",
    "    type: local\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  capacity:\n",
    "    storage: 1Gi\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  hostPath:\n",
    "    path: \"/data/logs\"\n",
    "\n",
    "---\n",
    "\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: airflow-data-volume\n",
    "  labels:\n",
    "    type: local\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  capacity:\n",
    "    storage: 1Gi\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  hostPath:\n",
    "    path: \"/data/data\"\n",
    "```\n",
    "\n",
    "Estes três objetos são responsáveis por criar os volumes persistentes: o airflow-dags-volume; o airflow-logs-volume; e o airflow-data-volume.\n",
    "\n",
    "Analisando apenas um deles (`airflow-dags-volume`), porque os outros dois estão defininindo e são basicamente são as mesmas configurações, mudando apenas o nome escolhido para o volume. \n",
    "\n",
    "```\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: airflow-dags-volume\n",
    "  labels:\n",
    "    type: local\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  capacity:\n",
    "    storage: 1Gi\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  hostPath:\n",
    "    path: \"/data/dags\"\n",
    "```\n",
    "\n",
    "Está sendo definindo um `kind`, que é `PersistentVolume`. Em `metadata`, é definido um nome para esse volume, que é `airflow-dags-volume`. Mais abaixo, é possível notar que em storage, será definido que o volume pode armazenar até 1Gi de dados.\n",
    "\n",
    "Além disso, em `accessModes`, definimos o modo `ReadWriteOnce`, que é um modo de leitura e escrita. Ele permite que apenas um node utilize o volume que estamos criando. No caso esse node será um minikube.\n",
    "\n",
    "Por fim, na parte de path, é definido o caminho onde o volume estará armazenado no minikube, isto é, na estrutura de pastas do minikube. Neste caso, será dentro de uma pasta chamada \"data\" e uma subpasta chamada \"dags\".\n",
    "\n",
    "Os outros objetos, tanto o quarto quanto o sexto, têm essa mesma configuração, mudando apenas o nome e o _path_.\n",
    "\n",
    "- **Terceiro, quinto e sétimo volumes:**\n",
    "\n",
    "```\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: airflow-dags-claim\n",
    "  namespace: airflow\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "\n",
    "---\n",
    "\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: airflow-logs-claim\n",
    "  namespace: airflow\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "\n",
    "---\n",
    "\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: airflow-data-claim\n",
    "  namespace: airflow\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "```\n",
    "\n",
    "Estes objetos são os objetos de reinvidicação de volume. Eles são chamados de volumes claim ou PVC (Persistent Volume Claim) e são responsáveis por pegar determinada parte do armazenamento disponível em um Persistent Volume e disponibilizá-lo para algum pod específico.\n",
    "\n",
    "Ou seja, um PVC é uma forma de um pod ter acesso à determinado espaço de armazenamento de um Persistent Volume (PV) específico. Por isso, é importante criar um PVC para cada PV criado.\n",
    "\n",
    "Como foram criados três PVs, é preciso criar três PVCs, que são três objetos de reinvidicação de volume. Ao analisar no nosso arquivo o terceiro objeto, que é um `VolumeClaim`, porque os outros objetos do mesmo tipo estão definindo basicamente as mesmas configurações.\n",
    "\n",
    "```\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: airflow-data-claim\n",
    "  namespace: airflow\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "```\n",
    "\n",
    "Foi definido: o `kind`, que é `PersistentVolumeClaim`; o `name`, que é `airflow-dags-claim`; e o `namespace`, que é onde esse _VolumeClaim_ será criado, no airflow (que foi criado como primeiro objeto desse arquivo).\n",
    "\n",
    "Além disso, também foi definido o `accessModes` como `ReadWriteOnce` e o `storage` com 1Gi. Isso significa que esse objeto pode reivindicar até 1Gi de armazenamento do Persistent Volume específico ao qual estiver vinculado. \n",
    "\n",
    "Tem o primeiro objeto, que está definindo o namespace. Em seguida, tem a definição do PersistentVolume `aiflow-dags-volume`. Embaixo dele, tem o `airflow-dags-claim`, que é o objeto de reinvidicação de volume do PersistentVolume `airflow-dags-volume`.\n",
    "\n",
    "> É muito importante que tudo esteja ordenado, para quando for criado definitivamente os volumes no kubernetes, ele vincule os PVs corretos aos PVCs corretos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Criando os volumes persistentes:**\n",
    "\n",
    "- Executar o Docker:\n",
    "\n",
    "`sudo service docker start`\n",
    "\n",
    "- Iniciar minikube:\n",
    "\n",
    "`minikube start`\n",
    "\n",
    "- Rodar o dashboard:\n",
    "\n",
    "`minikube dashboard`\n",
    "\n",
    "Enquanto está rodando, abrir outro terminal no diretório que está o seu-nome-do-arquivo-config.yml:\n",
    "\n",
    "- Aplicar configurações do arquivo-config.yml:\n",
    "\n",
    "`minikube kubectl -- apply -f airflow-kubernetes-config.yml`\n",
    "\n",
    "No _log_ de execução desse comando, apareceu que o _namespace_ foi criado, além dos volumes persistentes e dos objetos de reivindicação de volume, os PVCs. Para conferir se eles foram todos criados corretamente, pode acessar a dashboard do kubernetes (retornar ao terminal anterior, localizar e abrir o link).\n",
    "\n",
    "Para acessar os volumes é necessário, primeiro, acessar o namespace do airflow, que é onde foram criados. Pode alterar o namespace acessando a barra superior da dashboard, \"default\". Para isso é preciso apertar a seta para baixo e localizar o outro namespace que acabou de ser criado, o \"airflow\".\n",
    "\n",
    "Na barra lateral esqueda do dashboard, localizar o \"Persistent Volumes\" na seção de \"Cluster\". Ao selecionar, é possível checar os três volumes: airflow-dags-volume; airflow-data-volume; e airflow-logs-volume.\n",
    "\n",
    "À frente da cada um deles, é possível visualizar os objetos de reivindicação de volume, vinculados a cada um dos PVs, ou seja, os PVCs. \n",
    "\n",
    "É comum acontecer de os volumes claim estarem vinculados ao Persistent Volumes de forma incorreta. Um exemplo desse esse seria:\n",
    "\n",
    "columns: **Name** | Capacity | Access Mode | Reclaim Policy | Status | **Claim**\n",
    "problem: **Name: airflow-_dags_-volume** e **Claim: airflow-_logs_-claim** / **Name: airflow-_data_-volume** e **Claim: airflow-_dags_-claim** / **Name: airflow-_logs_-volume** e **Claim: airflow-_data_-claim**\n",
    "\n",
    "Esse problema pode acontecer mesmo tudo definido corretamente no arquivo YML. Para resolver é preciso:\n",
    "\n",
    "- Excluir todos os volumes claim, todos os volumes persistentes e criá-los novamente;\n",
    "- Na barra lateral esquerda, acessar \"**Persistent Volume Claims**\". É possível visualizar uma tabela \"**Persistent Volume Claims**\" e, ao final dela, \"três pontinhos\" na vertical. É preciso selecionar e apertar \"Delete\" para que sejam excluídos;\n",
    "- Voltar para a seção de \"**Persistent Volumes**\" e repetir o processo de exclusão;\n",
    "- Após excluídos, retornar ao terminal e executar novamente o comando: `minikube kubectl -- apply -f airflow-kubernetes-config.yml`.\n",
    "\n",
    "_Com os volumes criados, agora é preciso vincular cada um deles às pastas da máquina local_. Isso é importante para ter fácil acesso às informações e dados que foram salvos nos volumes pela máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Montando os volumes:**\n",
    "\n",
    "**Passo 1: criar as pastas que serão vinculadas**\n",
    "\n",
    "- Criar as pastas para vincular com os volumes. **Será feito no mesmo diretório que está o arquivo airflow-kubernetes-config.yml**\n",
    "\n",
    "`mkdir dags`\n",
    "\n",
    "`mkdir logs`\n",
    "\n",
    "`mkdir data`\n",
    "\n",
    "**Passo 2: montar pastas com os volumes criados**\n",
    "\n",
    "- Montar as pastas com cada um dos volumes específicos criados anteriormente. **O processo de montagem da pasta para o volume trava o terminal**, por isso abrir mais três terminais:\n",
    "\n",
    "`minikube mount ./dags/:/data/dags`\n",
    "\n",
    "`minikube mount ./logs/:/data/logs`\n",
    "\n",
    "`minikube mount ./data/:/data/data`\n",
    "\n",
    "**Passo 3: teste para conferir se está funcionando**\n",
    "\n",
    "- Voltar ao primeiro terminar sem nenhum processo que trave e executar: \n",
    "\n",
    "`minikube ssh`\n",
    "\n",
    "> Vai abrir - docker@minikube:\n",
    "\n",
    "- Acessar a pasta do minikube que dá acesso aos dados do volume airflow-dags-volume. Executar o comando `ll` e o caminho que deseja acessar `/data/dags`.\n",
    "\n",
    "`ll /data/dags`\n",
    "\n",
    "De acordo com o retorno, não tem nada nesse volume. Está correto, porque ainda não foi salvo nada nele. Mas, para testar se foi possível montar a pasta \"dags\" da máquina local para o volume, é necessário adicionar um arquivo na pasta de \"dags\" e verificar se ele aparecerá no volume. Para o teste é preciso abrir mais um terminal e acessar o diretório onde está o arquivo de airflow-kubernetes-config.yml. Em seguida criar o arquivo de teste:\n",
    "\n",
    "`code .`\n",
    "\n",
    "> quando feito pelo terminal direto, sem ser no vscode.\n",
    "\n",
    "Automaticamente, a pasta airflowalura foi aberta. É possível visualizar, ao lado direito, as subpastas. É preciso criar um arquivo dentro da pasta \"dags\". Para isso, adicionar em \"New file\" e chamar o arquivo de \"teste.txt\". Executar novamente o comando `ll /data/dags/` para conferir se esse arquivo adicionado irá aparecer no volume.\n",
    "\n",
    "Exemplo de output esperado:\n",
    "\n",
    "```\n",
    "    total 0\n",
    "\n",
    "    -rw-r--r-- 1 docker docker 0 Dec 27 15:20 teste.txt\n",
    "```\n",
    "\n",
    "Esse retorno diz que o volume não está mais vazio. Um arquivo foi criado e se chama \"test.txt\". Isso indica que o `mount` funcionou corretamente para a pasta 'dags' com o volume airflow-dags-volume. É posível fazer esse teste tb com as pastas 'logs' e 'datas'.\n",
    "\n",
    "Para sair do node do minikube no terminal, basta executar o comando:\n",
    "\n",
    "`exit`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
