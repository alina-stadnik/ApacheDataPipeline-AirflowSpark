{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explorando com Spark**\n",
    "\n",
    "**Objetivo: Transformação de dados (API do Twitter) com Spark**\n",
    "\n",
    "Essa primeira etapa consiste em:\n",
    "\n",
    "- **Instalar o PySpark e ler a base de dados brutos**;\n",
    "\n",
    "A instalação do PySpark é simples, mas deve prestar atenção aos pré-requisitos e versões utilizadas para não ter problema ao rodar alguns comandos.\n",
    "\n",
    "Depois de instalado, é possível ler os dados utilizando o SparkSession, onde é possível checar o tipo dos dados, que no caso é json, e a localização, que é dentro do Data Lake. A exploração fica por conta dos métodos show e printSchema.\n",
    "\n",
    "- **Identificar os dados que devem ser extraídos e simplificados**;\n",
    "\n",
    "Com os dados lidos e explorados, é possível identificar quais são os dados essenciais. No caso, existem os metadados que não vão ter utilidade para análise, então podem ser ignorados. Já os dados dos tweets e usuários são as informações mais importantes e precisam ser separadas, simplificando assim a leitura dos dados.\n",
    "\n",
    "- **Extrair e salvar os dados simplificados**;\n",
    "\n",
    "Utilizando os métodos select e explode é feita a extração dos dados de tweets e users, podendo salvá-los em um novo DataFrame. Com esses dois DataFrames a informação fica muito mais simples de ler e torna possível que a análise e decisão se vai ler ambos os dados ou não.\n",
    "\n",
    "- **Transformar todo esse processo em um script .py**.\n",
    "\n",
    "A última etapa é muito importante, pois é quando todas as decisões presentes nesse Jupyter Notebook são transformadas em um script que será incluso no pipeline orquestrado pelo Airflow. Para testar o Script, será utilizado a solução do SparkSubmit, que permite rodar código Spark de maneira simples e gerenciada pelo próprio Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.3.1\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing legacy 'setup.py install' for pyspark, since package 'wheel' is not installed.\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Uninstalling py4j-0.10.9.7:\n",
      "      Successfully uninstalled py4j-0.10.9.7\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.1\n",
      "    Uninstalling pyspark-3.5.1:\n",
      "      Successfully uninstalled pyspark-3.5.1\n",
      "  Running setup.py install for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Spark necessita do Java instalado na máquina para funcionar\n",
    "# no terminal: java -version\n",
    "# Caso não tenha instalado, no terminal: sudo apt-get install openjdk-8-jdk-headless -qq \n",
    "\n",
    "# pip install pyspark==3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"twitter_transformation\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") # reduce the amount of log output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# twitter_datascience tem subfolders com diferentes datas de extração\n",
    "parent_folder = \"../../datalake/twitter_datascience\"\n",
    "subfolders = [os.path.join(parent_folder, folder) for folder in os.listdir(parent_folder) if os.path.isdir(os.path.join(parent_folder, folder))]\n",
    "\n",
    "df = spark.read.json(subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|                data|            includes|              meta|\n",
      "+--------------------+--------------------+------------------+\n",
      "|[{44, 65, 2024-06...|{[{2024-06-02T19:...|{1234567890abcdef}|\n",
      "|[{71, 91, 2024-06...|{[{2024-06-02T06:...|{1234567890abcdef}|\n",
      "|[{44, 48, 2024-06...|{[{2024-06-02T05:...|{1234567890abcdef}|\n",
      "|[{25, 98, 2024-06...|{[{2024-06-02T16:...|{1234567890abcdef}|\n",
      "|[{23, 80, 2024-06...|{[{2024-06-02T07:...|{1234567890abcdef}|\n",
      "|[{82, 74, 2024-06...|{[{2024-06-02T04:...|              null|\n",
      "|[{90, 84, 2024-06...|{[{2024-06-01T14:...|{1234567890abcdef}|\n",
      "|[{8, 54, 2024-06-...|{[{2024-06-01T01:...|{1234567890abcdef}|\n",
      "|[{46, 97, 2024-06...|{[{2024-06-01T01:...|              null|\n",
      "|[{48, 67, 2024-05...|{[{2024-05-31T03:...|{1234567890abcdef}|\n",
      "|[{93, 43, 2024-05...|{[{2024-05-31T01:...|              null|\n",
      "|[{1, 27, 2024-05-...|{[{2024-05-29T22:...|              null|\n",
      "|[{34, 3, 2024-05-...|{[{2024-05-28T00:...|              null|\n",
      "|[{96, 49, 2024-05...|{[{2024-05-30T03:...|              null|\n",
      "+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- author_id: string (nullable = true)\n",
      " |    |    |-- conversation_id: string (nullable = true)\n",
      " |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |-- edit_history_tweet_ids: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- in_reply_to_user_id: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- public_metrics: struct (nullable = true)\n",
      " |    |    |    |-- like_count: long (nullable = true)\n",
      " |    |    |    |-- quote_count: long (nullable = true)\n",
      " |    |    |    |-- reply_count: long (nullable = true)\n",
      " |    |    |    |-- retweet_count: long (nullable = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- includes: struct (nullable = true)\n",
      " |    |-- users: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- username: string (nullable = true)\n",
      " |-- meta: struct (nullable = true)\n",
      " |    |-- next_token: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 1: Extrair informações da coluna data\n",
    "\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col: struct (nullable = true)\n",
      " |    |-- author_id: string (nullable = true)\n",
      " |    |-- conversation_id: string (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- edit_history_tweet_ids: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- in_reply_to_user_id: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- public_metrics: struct (nullable = true)\n",
      " |    |    |-- like_count: long (nullable = true)\n",
      " |    |    |-- quote_count: long (nullable = true)\n",
      " |    |    |-- reply_count: long (nullable = true)\n",
      " |    |    |-- retweet_count: long (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transforma as informações da coluna data em linhas dentro do df atual\n",
    "\n",
    "df.select(f.explode('data')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 col|\n",
      "+--------------------+\n",
      "|{44, 65, 2024-06-...|\n",
      "|{78, 46, 2024-06-...|\n",
      "|{76, 98, 2024-06-...|\n",
      "|{88, 47, 2024-06-...|\n",
      "|{66, 2, 2024-06-0...|\n",
      "|{78, 86, 2024-06-...|\n",
      "|{37, 89, 2024-06-...|\n",
      "|{62, 8, 2024-06-0...|\n",
      "|{34, 29, 2024-06-...|\n",
      "|{53, 45, 2024-06-...|\n",
      "|{71, 91, 2024-06-...|\n",
      "|{6, 47, 2024-06-0...|\n",
      "|{95, 99, 2024-06-...|\n",
      "|{13, 32, 2024-06-...|\n",
      "|{99, 97, 2024-06-...|\n",
      "|{5, 52, 2024-06-0...|\n",
      "|{40, 55, 2024-06-...|\n",
      "|{26, 55, 2024-06-...|\n",
      "|{10, 95, 2024-06-...|\n",
      "|{95, 64, 2024-06-...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.explode('data')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ainda tem apenas uma única coluna com um json de conteúdo das rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "|author_id|conversation_id|          created_at| id|like_count|quote_count|reply_count|retweet_count|                text|\n",
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "|       44|             65|2024-06-02T21:15:...| 58|        99|         10|         25|           68|Este é um tweet f...|\n",
      "|       78|             46|2024-06-02T17:40:...| 14|        57|         84|         59|           30|Tweet fictício ge...|\n",
      "|       76|             98|2024-06-02T15:10:...| 16|        75|         35|         91|           40|Tweet fictício ge...|\n",
      "|       88|             47|2024-06-02T09:58:...| 10|        42|         26|         42|           10|Um terceiro tweet...|\n",
      "|       66|              2|2024-06-02T18:36:...|  8|        67|         96|         95|           43|Tweet fictício ge...|\n",
      "|       78|             86|2024-06-02T09:31:...| 68|         3|         43|         35|           78|Este é um tweet f...|\n",
      "|       37|             89|2024-06-02T01:30:...| 36|        51|         85|         35|           26|Este é um tweet f...|\n",
      "|       62|              8|2024-06-02T07:39:...|  8|        94|         54|         34|           31|Um terceiro tweet...|\n",
      "|       34|             29|2024-06-02T04:02:...| 98|        31|         12|          6|           55|Tweet fictício ge...|\n",
      "|       53|             45|2024-06-02T19:24:...| 41|        71|         49|         13|           44|Outro tweet fictí...|\n",
      "|       71|             91|2024-06-02T19:30:...| 78|        27|         27|         65|           95|Tweet fictício ge...|\n",
      "|        6|             47|2024-06-02T14:04:...|  9|        60|         53|         55|           27|Este é um tweet f...|\n",
      "|       95|             99|2024-06-02T20:41:...| 75|        77|          1|         76|            2|Este é um tweet f...|\n",
      "|       13|             32|2024-06-02T15:00:...| 74|        19|         35|         87|           60|Este é um tweet f...|\n",
      "|       99|             97|2024-06-02T07:30:...| 93|        26|         29|         78|           38|Tweet fictício ge...|\n",
      "|        5|             52|2024-06-02T19:26:...| 39|        35|          3|         36|           71|Outro tweet fictí...|\n",
      "|       40|             55|2024-06-02T21:11:...| 93|        73|         85|         40|            9|Tweet fictício cr...|\n",
      "|       26|             55|2024-06-02T14:02:...| 17|        24|         91|         98|           42|Tweet fictício ge...|\n",
      "|       10|             95|2024-06-02T08:42:...| 38|         0|         33|         96|           54|Um terceiro tweet...|\n",
      "|       95|             64|2024-06-02T14:03:...| 81|        80|         54|         49|           49|Tweet fictício ge...|\n",
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformação dos dados em colunas\n",
    "\n",
    "df.select(f.explode('data').alias('tweets')).select(\"tweets.author_id\", \"tweets.conversation_id\",\n",
    "        \"tweets.created_at\", \"tweets.id\",\n",
    "        \"tweets.public_metrics.*\", \"tweets.text\").show()\n",
    "\n",
    "# public_metrics recebe * pois ainda não está com uma estrutura simplificada e isso permite acessar todos os campos que pertencem a essa métrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- conversation_id: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- like_count: long (nullable = true)\n",
      " |-- quote_count: long (nullable = true)\n",
      " |-- reply_count: long (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.explode(\"data\").alias(\"tweets\"))\\\n",
    ".select(\"tweets.author_id\", \"tweets.conversation_id\",\n",
    "        \"tweets.created_at\", \"tweets.id\",\n",
    "        \"tweets.public_metrics.*\", \"tweets.text\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salva as informações em uma variável\n",
    "tweet_df = df.select(f.explode(\"data\").alias(\"tweets\"))\\\n",
    ".select(\"tweets.author_id\", \"tweets.conversation_id\",\n",
    "        \"tweets.created_at\", \"tweets.id\",\n",
    "        \"tweets.public_metrics.*\", \"tweets.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "|author_id|conversation_id|          created_at| id|like_count|quote_count|reply_count|retweet_count|                text|\n",
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "|       44|             65|2024-06-02T21:15:...| 58|        99|         10|         25|           68|Este é um tweet f...|\n",
      "|       78|             46|2024-06-02T17:40:...| 14|        57|         84|         59|           30|Tweet fictício ge...|\n",
      "|       76|             98|2024-06-02T15:10:...| 16|        75|         35|         91|           40|Tweet fictício ge...|\n",
      "|       88|             47|2024-06-02T09:58:...| 10|        42|         26|         42|           10|Um terceiro tweet...|\n",
      "|       66|              2|2024-06-02T18:36:...|  8|        67|         96|         95|           43|Tweet fictício ge...|\n",
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json -> dataframe: mais fácil e simples para acessar os dados\n",
    "tweet_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando um novo DataFrame para os dados dos usuários que fizeram esses tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 col|\n",
      "+--------------------+\n",
      "|{2024-06-02T19:47...|\n",
      "|{2024-06-02T16:33...|\n",
      "|{2024-06-02T09:12...|\n",
      "|{2024-06-02T05:57...|\n",
      "|{2024-06-02T11:53...|\n",
      "|{2024-06-02T17:45...|\n",
      "|{2024-06-02T02:00...|\n",
      "|{2024-06-02T19:17...|\n",
      "|{2024-06-02T20:51...|\n",
      "|{2024-06-02T17:41...|\n",
      "|{2024-06-02T06:50...|\n",
      "|{2024-06-02T16:59...|\n",
      "|{2024-06-02T05:56...|\n",
      "|{2024-06-02T17:41...|\n",
      "|{2024-06-02T21:52...|\n",
      "|{2024-06-02T21:54...|\n",
      "|{2024-06-02T09:21...|\n",
      "|{2024-06-02T09:26...|\n",
      "|{2024-06-02T04:17...|\n",
      "|{2024-06-02T18:16...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformar a lista de usuários em linhas do DataFrame\n",
    "df.select(f.explode(\"includes.users\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               users|\n",
      "+--------------------+\n",
      "|{2024-06-02T19:47...|\n",
      "|{2024-06-02T16:33...|\n",
      "|{2024-06-02T09:12...|\n",
      "|{2024-06-02T05:57...|\n",
      "|{2024-06-02T11:53...|\n",
      "|{2024-06-02T17:45...|\n",
      "|{2024-06-02T02:00...|\n",
      "|{2024-06-02T19:17...|\n",
      "|{2024-06-02T20:51...|\n",
      "|{2024-06-02T17:41...|\n",
      "|{2024-06-02T06:50...|\n",
      "|{2024-06-02T16:59...|\n",
      "|{2024-06-02T05:56...|\n",
      "|{2024-06-02T17:41...|\n",
      "|{2024-06-02T21:52...|\n",
      "|{2024-06-02T21:54...|\n",
      "|{2024-06-02T09:21...|\n",
      "|{2024-06-02T09:26...|\n",
      "|{2024-06-02T04:17...|\n",
      "|{2024-06-02T18:16...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nomeando a nova coluna de usuário usando o alias\n",
    "df.select(f.explode(\"includes.users\").alias(\"users\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utiliza o select e * para selecionar todos os campos de informações dos usuários\n",
    "# printSchema para verificar o formato do DataFrame\n",
    "df.select(f.explode(\"includes.users\").alias(\"users\")).select(\"users.*\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva em uma variável\n",
    "user_df = df.select(f.explode(\"includes.users\").alias(\"users\")).select(\"users.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------+--------+\n",
      "|          created_at| id|  name|username|\n",
      "+--------------------+---+------+--------+\n",
      "|2024-06-02T19:47:...| 38|User 1|   user1|\n",
      "|2024-06-02T16:33:...|  9|User 2|   user2|\n",
      "|2024-06-02T09:12:...| 58|User 3|   user3|\n",
      "|2024-06-02T05:57:...| 53|User 4|   user4|\n",
      "|2024-06-02T11:53:...|  2|User 5|   user5|\n",
      "+--------------------+---+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Salvando as estruturas criadas\n",
    "tweet_df.coalesce(1).write.mode(\"overwrite\").json('output/tweet')\n",
    "user_df.coalesce(1).write.mode(\"overwrite\").json('output/user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `.coalesce(1)` com o parâmetro 1 é responsável por pegar as informações que o Spark quebra em pedaços (nodes) e as une permitindo que sejam salvas em um só arquivo.\n",
    "\n",
    "> Com um grande volume de dados é preferível salvar eles em diversos arquivos para não estourar a memória do node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparando o ambiente: instalando Spark e SparkSubmit**\n",
    "\n",
    "O Spark precisa estar na versão 3.1.3 e a versão 3.2 do Hadoop. No terminal, pelo comando **wget** com a URL de download:\n",
    "\n",
    "`wget https://archive.apache.org/dist/spark/spark-3.1.3/spark-3.1.3-bin-hadoop3.2.tgz`\n",
    "\n",
    "Depois de terminado o download é necessário descompactar o arquivo através do comando tar, passando por parâmetros as configurações -xvzf e o nome do arquivo.\n",
    "\n",
    "`tar -xvzf spark-3.1.3-bin-hadoop3.2.tgz`\n",
    "\n",
    "A partir disso já é possível gerar o script desenvolvido (transformation.py). Para isso é necessário entrar no dir descompactado (usar comando ls para verificar ele para então usar o comando cd).\n",
    "\n",
    "`cd spark-3.1.3-bin-hadoop3.2`\n",
    "\n",
    "`./bin/spark-submit` (executa no terminal, dentro da pasta descompactado, o caminho bin e o arquivo spark-submit)\n",
    "\n",
    "> Esse executável recebe como parâmetro a localização do script a ser executado, que no caso é o caminho até o arquivo transformation.py.\n",
    "\n",
    "`./bin/spark-submit …/src/spark/transformation.py`\n",
    "\n",
    "**Também é preciso passar mais três parâmetros, _que são pré requisitos do script criado_. No caso os parâmetros são: src, destino e process-date.**\n",
    "\n",
    "`./bin/spark-submit …/src/spark/transformation.py --src --destino --process-date`\n",
    "\n",
    "- Para o src coloca o caminho até os dados brutos no data lake (parent folder com subdiretórios);\n",
    "- Para o destino coloca dentro de uma pasta temporária de output;\n",
    "- O process-date pode colocar a data em que está rodando o processo.\n",
    "\n",
    "`./bin/spark-submit …/src/spark/transformation.py --src …/datalake/twitter_datasciencie --destino …/src/spark --process-date 2024-07-04`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fazendo o Airflow se comunicar com o Spark Submit:**\n",
    "\n",
    "- Instalar o modulo Spark;\n",
    "- Utilizar o SparkSubmitOperator;\n",
    "- Atualizar uma DAG;\n",
    "- Salvar os dados transformados pelo Spark.\n",
    "\n",
    "Com o ambiente virtual ativo, é preciso executar o seguinte código para instalar o operador:\n",
    "\n",
    "`pip install apache-airflow-providers-apache-spark`\n",
    "\n",
    "Ao final da instação é possível atualizar a DAG de interesse do Airflow com a seguinte importação:\n",
    "\n",
    "`from airflow.providers.apache.spark.operators.spark_submit`\n",
    "\n",
    "### **Inicializando o Airflow:**\n",
    "\n",
    "**IMPORTANTE**: Exportar a segunda variável de ambiente:\n",
    "\n",
    "`export SPARK_HOME=/home/pacer/Documents/Cursos-Alura/spark-3.1.3-bin-hadoop3.2` (passar onde está salvo essa pasta do Spark)\n",
    "\n",
    "`airflow standalone`\n",
    "\n",
    "Quando o Airflow iniciar (na porta :8080 - \"localhost:8080/home\").\n",
    "\n",
    "Em **\"Admin\"**, na barra superior, seleciona a opção **\"Connections\"**, mas, ao invés de criar uma nova conexão, é possível apenas **editar** uma já existente: a do **Spark**, que foi *trazida com a instalação dos provides do Spark*.\n",
    "\n",
    "Usa o \"Ctrl + F\", digite \"spark\" para buscar essa ocorrência na listagem e selecionar o botão \"Edit record\" à esquerda de \"spark_default\". O nome deve permanecer \"spark_default\"; enquanto o tipo deve ser \"Spark\"; e o host, \"local\". Após isso, selecionar salvar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
