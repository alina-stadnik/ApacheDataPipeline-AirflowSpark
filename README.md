# Apache Airflow e Apache Spark

Bem-vindo ao repositório de estudos sobre [Apache Airflow](https://airflow.apache.org/docs/apache-airflow/2.3.2/index.html) e [Apache Spark](https://spark.apache.org/docs/latest/). Este repositório contém uma coleção de projetos, exemplos e documentação que exploram o uso dessas duas ferramentas de processamento e orquestração de dados.

## **Apache Airflow**

O Apache Airflow é uma plataforma para criar, agendar e monitorar fluxos de trabalho de forma programática. Quando os fluxos de trabalho são definidos como código, eles se tornam mais fáceis de manter, versionáveis, testáveis e colaborativos.

### Princípios do Airflow:

- **Dinâmico**: Os pipelines do Airflow são configurados como código (Python), permitindo a geração dinâmica de pipelines.
- **Extensível**: Permite definir facilmente operadores próprios e executores, ajustando-se ao nível de abstração necessário.
- **Elegante**: Pipelines enxutos e explícitos, com parametrização incorporada ao núcleo do Airflow usando o mecanismo de modelagem Jinja.
- **Escalável**: Arquitetura modular que utiliza filas de mensagens para orquestrar um número arbitrário de trabalhadores.

## **Apache Spark**

O Apache Spark é uma estrutura de processamento de dados em grande escala que permite a execução rápida de tarefas em paralelo. Ele fornece uma interface abrangente para trabalhar com dados, oferecendo suporte a várias linguagens, incluindo Python.

### Princípios do Spark:

- **Rápido**: Otimizado para desempenho, utilizando processamento em memória sempre que possível.
- **Versátil**: Oferece uma ampla gama de bibliotecas para tarefas de análise de dados, incluindo Spark SQL, Spark Streaming e MLlib.
- **Escalável**: Permite o processamento de grandes volumes de dados, em clusters locais ou na nuvem.
- **Tolerante a Falhas**: Modelo de programação resiliente que garante a recuperação automática em caso de falhas.

## Estrutura do Repositório

Este repositório contém:

- **Projetos**: Exemplos práticos de uso do Airflow e do Spark em diferentes contextos.
- **Documentação**: Instruções de instalação, configuração e melhores práticas para trabalhar com Airflow e Spark.
- **Notas de Estudo**: Anotações e reflexões sobre conceitos aprendidos durante os estudos.